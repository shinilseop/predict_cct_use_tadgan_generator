{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e81843-a5c4-4f9e-a7e6-3ad9980473c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def _wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)\n",
    "\n",
    "class TadGAN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    TadGAN based on the idea of https://arxiv.org/pdf/2009.07769.pdf\n",
    "\n",
    "    The model can be used for anomaly detection in univariate & multivariate time series based on a GAN network architecture.\n",
    "\n",
    "    This GAN architecture uses a encoder-generator network in combination with two discriminator networks (critics).\n",
    "    One for the encoding of the incoming time series and the other to discriminate the learned embedding.\n",
    "\n",
    "    This implementation uses the new TensorFlow 2 / Keras Subclassing API instead of a simple Python class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Time series hyper parameters\n",
    "        ts_input_shape: Tuple[int] = (100, 1),\n",
    "        latent_dim: int = 20,\n",
    "        gradient_penelty_weight: int = 10,\n",
    "        n_iterations_critic: int = 5,\n",
    "\n",
    "        # sub network hyper parameters\n",
    "        encoder_lstm_units: int = 100,\n",
    "        generator_lstm_units: int = 100,\n",
    "        generator_output_activation: str = \"tanh\",\n",
    "        critic_x_cnn_blocks: int = 4,\n",
    "        critic_x_cnn_filters: int = 64,\n",
    "        critic_z_dense_units: int = 100,\n",
    "\n",
    "        log_all_losses: bool = True,\n",
    "        print_model_summaries: bool = False\n",
    "    ):\n",
    "        super(TadGAN, self).__init__()\n",
    "\n",
    "        # Parse default variables\n",
    "        self.latent_dim = latent_dim\n",
    "        self.ts_input_shape = ts_input_shape\n",
    "        self.signal_length = ts_input_shape[0]\n",
    "        self.n_channels = ts_input_shape[1]\n",
    "        self.gradient_penelty_weight = gradient_penelty_weight\n",
    "        self.n_iterations_critic = n_iterations_critic\n",
    "        self.log_all_losses = log_all_losses\n",
    "\n",
    "        # TadGAN encoder\n",
    "        self.encoder_lstm_units = encoder_lstm_units\n",
    "        self.encoder = self._build_encoder(lstm_units=self.encoder_lstm_units)\n",
    "\n",
    "        # TadGAN generator\n",
    "        self.generator_lstm_units = generator_lstm_units\n",
    "        self.generator_act_fn = generator_output_activation\n",
    "        self.generator = self._build_generator(generator_lstm_units=self.generator_lstm_units, output_activation=generator_output_activation)\n",
    "\n",
    "        # TadGAN critic x\n",
    "        self.critic_x_cnn_filters = critic_x_cnn_filters\n",
    "        self.critic_x_cnn_blocks = critic_x_cnn_blocks\n",
    "        self.critic_x = self._build_critic_x(n_cnn_filters=self.critic_x_cnn_filters, n_cnn_blocks=self.critic_x_cnn_blocks)\n",
    "\n",
    "        # TadGAN critic z\n",
    "        self.critic_z_dense_units = critic_z_dense_units\n",
    "        self.critic_z = self._build_critic_z(critic_z_dense_units=self.critic_z_dense_units)\n",
    "\n",
    "        if print_model_summaries:\n",
    "            print(self.encoder.summary())\n",
    "            print(self.generator.summary())\n",
    "            print(self.critic_x.summary())\n",
    "            print(self.critic_z.summary())\n",
    "\n",
    "        self.build(input_shape=(None, ts_input_shape[0], ts_input_shape[1]))\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        \"\"\"\n",
    "        Build a config dict for the custom attributes of this subclassing Keras model\n",
    "\n",
    "        :return: Config as Python dict\n",
    "        \"\"\"\n",
    "        return dict(\n",
    "            ts_input_shape=self.ts_input_shape,\n",
    "            latent_dim=self.latent_dim,\n",
    "            gradient_penelty_weight=self.gradient_penelty_weight,\n",
    "            n_iterations_critic=self.n_iterations_critic,\n",
    "            log_all_losses=self.log_all_losses,\n",
    "\n",
    "            # Hyperparameters\n",
    "            encoder_lstm_units=self.encoder_lstm_units,\n",
    "            generator_lstm_units=self.generator_lstm_units,\n",
    "            critic_x_cnn_filters=self.critic_x_cnn_filters,\n",
    "            critic_x_cnn_blocks=self.critic_x_cnn_blocks,\n",
    "            critic_z_dense_units=self.critic_z_dense_units\n",
    "        )\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        critic_x_optimizer,\n",
    "        critic_z_optimizer,\n",
    "        encoder_generator_optimizer,\n",
    "        critic_x_loss_fn,\n",
    "        critic_z_loss_fn,\n",
    "        encoder_generator_loss_fn,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Customized Keras model compilation based on the idea of https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "        :param critic_x_optimizer: Keras optimizer for the critic x model\n",
    "        :param critic_z_optimizer: Keras optimizer for the critic z model\n",
    "        :param encoder_generator_optimizer: Keras optimizer for the encoder & generator model\n",
    "        :param critic_x_loss_fn: Loss function for the critic x model\n",
    "        :param critic_z_loss_fn: Loss function for the critic z model\n",
    "        :param encoder_generator_loss_fn: Loss function for the generator & encoder model\n",
    "\n",
    "        :param kwargs: Additional kwargs forwarded to the super class\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        super(TadGAN, self).compile(**kwargs)\n",
    "\n",
    "        self.critic_x_optimizer = critic_x_optimizer\n",
    "        self.critic_z_optimizer = critic_z_optimizer\n",
    "        self.encoder_generator_optimizer = encoder_generator_optimizer\n",
    "\n",
    "        self.critic_x_loss_fn = critic_x_loss_fn\n",
    "        self.critic_z_loss_fn = critic_z_loss_fn\n",
    "        self.encoder_generator_loss_fn = encoder_generator_loss_fn\n",
    "\n",
    "    def _build_encoder(self, lstm_units: int = 100):\n",
    "        \"\"\"\n",
    "        Build the Encoder subnetwork for the GAN. This model learns the compressed representation of the input\n",
    "        time series.\n",
    "        The encoder uses a single layer BI-LSTM network to learn the compressed representation.\n",
    "        The number of LSTM units can be adjusted.\n",
    "\n",
    "        :param lstm_units: Number of LSTM units that could be used for the time series encoding\n",
    "\n",
    "        :return: Encoder model\n",
    "        \"\"\"\n",
    "        x = tf.keras.layers.Input(shape=self.ts_input_shape, name=\"encoder_input\")\n",
    "\n",
    "        # Encode the sequence and extend its dimensions\n",
    "        encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_units, return_sequences=True))(x)\n",
    "        encoded = tf.keras.layers.Flatten()(encoded)\n",
    "        encoded = tf.keras.layers.Dense(units=self.latent_dim, name=\"latent_encoding\")(encoded)\n",
    "        encoded = tf.keras.layers.Reshape(target_shape=(self.latent_dim, 1), name='output_encoder')(encoded)\n",
    "\n",
    "        model = tf.keras.Model(inputs=x, outputs=encoded, name=\"encoder_model\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_generator(self, generator_lstm_units: int = 100, output_activation: str = \"tanh\") -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Build the Generator model for the GAN. This model uses the compressed representation of the encoder and\n",
    "        tries to reconstruct the original time series from it.\n",
    "        At the moment a two-layer Bi-LSTM network is used for the reconstruction.\n",
    "\n",
    "        :param generator_lstm_units: Number of LSTM units that should be used for the reconstruction.\n",
    "        :param output_activation: The final activation of the generator. Choose with respect to the preprocesssing (scaling/normalizing)\n",
    "                       of the input time series\n",
    "\n",
    "        :return: Generator model\n",
    "        \"\"\"\n",
    "        x = tf.keras.layers.Input(shape=(self.latent_dim, 1), name=\"generator_input\")\n",
    "\n",
    "        # Remove additional dimensions from the latent embedding\n",
    "        decoded = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "        # Check if the sequence length is a even number (this is required for this model architecture)\n",
    "        if self.signal_length % 2 == 1:\n",
    "            raise ValueError(f\"The signal length needs to be even (current signal length: {self.signal_length})\")\n",
    "\n",
    "        # Build the first layer of the generator that should be half the size of the sequence length\n",
    "        half_seq_length = self.signal_length // 2\n",
    "        decoded = tf.keras.layers.Dense(units=half_seq_length)(decoded)\n",
    "        decoded = tf.keras.layers.Reshape(target_shape=(half_seq_length, 1))(decoded)\n",
    "\n",
    "        # Generation of a new time series using LSTM in combination with up sampling\n",
    "        decoded = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(units=generator_lstm_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            merge_mode='concat'\n",
    "        )(decoded)\n",
    "        decoded = tf.keras.layers.UpSampling1D(2)(decoded)\n",
    "        decoded = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(units=generator_lstm_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            merge_mode='concat'\n",
    "        )(decoded)\n",
    "\n",
    "        # Rebuild the original time series signal for all channels\n",
    "        decoded = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.n_channels))(decoded)\n",
    "        decoded = tf.keras.layers.Activation(activation=output_activation)(decoded)\n",
    "\n",
    "        model = tf.keras.Model(inputs=x, outputs=decoded, name=\"generator_model\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_critic_x(self, n_cnn_filters: int = 64, n_cnn_blocks: int = 4) -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Build the critic x network that learns to differ between a fake and real input sequence. The classifier uses a\n",
    "        stack of 1D CNN block (CNN + leaky relu + dropout) and a final fully connected classifier network.\n",
    "\n",
    "        :return: Critic x model\n",
    "        \"\"\"\n",
    "\n",
    "        x = tf.keras.layers.Input(shape=self.ts_input_shape, name=\"critic_x_input\")\n",
    "\n",
    "        if n_cnn_blocks < 1:\n",
    "            raise ValueError(f\"The number of CNN blocks needs to be greater than 1 (current value: {n_cnn_blocks})\")\n",
    "\n",
    "        y = tf.keras.layers.Conv1D(filters=n_cnn_filters, kernel_size=5)(x)\n",
    "        y = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
    "        y = tf.keras.layers.Dropout(rate=0.25)(y)\n",
    "\n",
    "        if n_cnn_blocks > 1:\n",
    "            for i in range(n_cnn_blocks - 1):\n",
    "                y = tf.keras.layers.Conv1D(filters=n_cnn_filters, kernel_size=5)(y)\n",
    "                y = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
    "                y = tf.keras.layers.Dropout(rate=0.25)(y)\n",
    "\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "        y = tf.keras.layers.Dense(1)(y)\n",
    "\n",
    "        model = tf.keras.Model(inputs=x, outputs=y, name=\"critic_x_model\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_critic_z(self, critic_z_dense_units: int = 100) -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Build the critic z model that learns to differ between a real and a fake encoding coming from the encoder.\n",
    "        The network works with a two-layer fully connected network in combination with a leaky RELU activation\n",
    "        and dropout regularization.\n",
    "\n",
    "        :param critic_z_dense_units: Number of units for each of the fully connected layers\n",
    "\n",
    "        :return: Critic z model\n",
    "        \"\"\"\n",
    "\n",
    "        x = tf.keras.layers.Input(shape=(self.latent_dim, 1), name=\"critic_z_input\")\n",
    "        y = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "        y = tf.keras.layers.Dense(units=critic_z_dense_units)(y)\n",
    "        y = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
    "        y = tf.keras.layers.Dropout(rate=0.2)(y)\n",
    "\n",
    "        y = tf.keras.layers.Dense(units=critic_z_dense_units)(y)\n",
    "        y = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
    "        y = tf.keras.layers.Dropout(rate=0.2)(y)\n",
    "\n",
    "        y = tf.keras.layers.Dense(1)(y)\n",
    "\n",
    "        model = tf.keras.Model(inputs=x, outputs=y, name=\"critic_z_model\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @tf.function\n",
    "    def critic_x_gradient_penalty(self, batch_size, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the gradient penalty.\n",
    "        \"\"\"\n",
    "        alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "        interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.critic_x(interpolated)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "        return gp\n",
    "\n",
    "    @tf.function\n",
    "    def critic_z_gradient_penalty(self, batch_size, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the gradient penalty.\n",
    "        \"\"\"\n",
    "        alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "        interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.critic_z(interpolated)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((1.0 - norm) ** 2)\n",
    "\n",
    "        return gp\n",
    "\n",
    "    @tf.function\n",
    "    def _critic_x_loss(self, x_mb, z, valid, fake, mini_batch_size):\n",
    "        \"\"\"\n",
    "        Do a step forward and calculate the loss on the critic x model\n",
    "\n",
    "        :param x_mb: Minibatch of input data\n",
    "        :param z: Minibatch of random noise\n",
    "        :param valid: Ground truth vector for valid samples\n",
    "        :param fake: Ground truth vector for fake samples\n",
    "        :param mini_batch_size:\n",
    "\n",
    "        :return: A tuple containing the total loss and the three single losses\n",
    "        \"\"\"\n",
    "        # Do a step forward on critic x model and collect gradients\n",
    "        x_ = self.generator(z)\n",
    "        fake_x = self.critic_x(x_)\n",
    "        valid_x = self.critic_x(x_mb)\n",
    "\n",
    "        # Calculate critic x loss\n",
    "        critic_x_valid_cost = self.critic_x_loss_fn(y_true=valid, y_pred=valid_x)\n",
    "        critic_x_fake_cost = self.critic_x_loss_fn(y_true=fake, y_pred=fake_x)\n",
    "        # TODO: [SMe] Is the mini_batch size still required?\n",
    "        critic_x_gradient_penalty = self.critic_x_gradient_penalty(mini_batch_size, x_mb, x_)\n",
    "        critic_x_total_loss = critic_x_valid_cost + critic_x_fake_cost + (critic_x_gradient_penalty * self.gradient_penelty_weight)\n",
    "\n",
    "        return critic_x_total_loss, critic_x_valid_cost, critic_x_fake_cost, critic_x_gradient_penalty\n",
    "\n",
    "    @tf.function\n",
    "    def _critic_z_loss(self, x_mb, z, valid, fake, mini_batch_size):\n",
    "        \"\"\"\n",
    "        Do a step forward and calculate the loss on the critic z model\n",
    "\n",
    "        :param x_mb: Minibatch of input data\n",
    "        :param z: Minibatch of random noise\n",
    "        :param valid: Ground truth vector for valid samples\n",
    "        :param fake: Ground truth vector for fake samples\n",
    "        :param mini_batch_size:\n",
    "\n",
    "        :return: A tuple containing the total loss and the three single losses\n",
    "        \"\"\"\n",
    "        # Do a step forward on critic z model and collect gradients\n",
    "        z_ = self.encoder(x_mb)\n",
    "        fake_z = self.critic_z(z_)\n",
    "        valid_z = self.critic_z(z)\n",
    "\n",
    "        # Calculate critic z loss\n",
    "        critic_z_valid_cost = self.critic_z_loss_fn(y_true=valid, y_pred=valid_z)\n",
    "        critic_z_fake_cost = self.critic_z_loss_fn(y_true=fake, y_pred=fake_z)\n",
    "        critic_z_gradient_penalty = self.critic_z_gradient_penalty(mini_batch_size, z, z_)\n",
    "        critic_z_total_loss = critic_z_valid_cost + critic_z_fake_cost + (critic_z_gradient_penalty * self.gradient_penelty_weight)\n",
    "\n",
    "        return critic_z_total_loss, critic_z_valid_cost, critic_z_fake_cost, critic_z_gradient_penalty\n",
    "\n",
    "    @tf.function\n",
    "    def _encoder_generator_loss(self, x_mb, z, valid):\n",
    "        \"\"\"\n",
    "        Do a step forward and calculate the loss on the encoder-generator model\n",
    "\n",
    "        :param x_mb: Minibatch of input data\n",
    "        :param z: Minibatch of random noise\n",
    "        :param valid: Ground truth vector for valid samples\n",
    "\n",
    "        :return: A tuple containing the total loss and the three single losses\n",
    "        \"\"\"\n",
    "        # Do a step forward on the encoder generator model\n",
    "        x_gen_ = self.generator(z)\n",
    "        fake_gen_x = self.critic_x(x_gen_)\n",
    "\n",
    "        z_gen_ = self.encoder(x_mb)\n",
    "        x_gen_rec = self.generator(z_gen_)\n",
    "        fake_gen_z = self.critic_z(z_gen_)\n",
    "\n",
    "        # Calculate encoder generator loss\n",
    "        encoder_generator_fake_gen_x_cost = self.encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_x)\n",
    "        encoder_generator_fake_gen_z_cost = self.encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_z)\n",
    "\n",
    "        # Use simple MSE as reconstruction error\n",
    "        general_reconstruction_cost = tf.reduce_mean(tf.square((x_mb - x_gen_rec)))\n",
    "        encoder_generator_total_loss = encoder_generator_fake_gen_x_cost + encoder_generator_fake_gen_z_cost + (10.0 * general_reconstruction_cost)\n",
    "\n",
    "        return encoder_generator_total_loss, encoder_generator_fake_gen_x_cost, encoder_generator_fake_gen_z_cost, general_reconstruction_cost\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, X) -> dict:\n",
    "        \"\"\"\n",
    "        Custom training step for this Subclassing API Keras model.\n",
    "        The shape should be (n_iterations_critic * batch size, n_channels) because the critic networks are trained\n",
    "        multiple times over the encoder-generator network.\n",
    "\n",
    "        :param X: Group of mini batches that are used to train the critics and the encoder-generator network\n",
    "                  Shape: (batch_size, signal_length, n_channels)\n",
    "\n",
    "        :return: Sub-model losses as los dict\n",
    "        \"\"\"\n",
    "        if isinstance(X, tuple):\n",
    "            X = X[0]\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        mini_batch_size = batch_size // self.n_iterations_critic\n",
    "\n",
    "        # Prepare the ground truth data\n",
    "        fake = tf.ones((mini_batch_size, 1))\n",
    "        valid = -tf.ones((mini_batch_size, 1))\n",
    "\n",
    "        critic_x_loss_steps = []\n",
    "        critic_z_loss_steps = []\n",
    "\n",
    "        # Train the critics multiple steps more then the encoder-generator model\n",
    "        for critic_train_step in range(self.n_iterations_critic):\n",
    "            z = tf.random.normal(shape=(mini_batch_size, self.latent_dim, 1))\n",
    "            x_mb = X[critic_train_step * mini_batch_size: (critic_train_step + 1) * mini_batch_size]\n",
    "\n",
    "            # Optimize step on critic x\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Do a step forward on critic x model and collect gradients\n",
    "                _critic_x_losses = self._critic_x_loss(x_mb, z, valid, fake, mini_batch_size)\n",
    "\n",
    "            # Backward step with updating critic x weights\n",
    "            critic_x_gradient = tape.gradient(_critic_x_losses[0], self.critic_x.trainable_variables)\n",
    "            self.critic_x_optimizer.apply_gradients(zip(critic_x_gradient, self.critic_x.trainable_variables))\n",
    "\n",
    "            # Collect summaries for logging\n",
    "            _critic_x_losses = np.array(_critic_x_losses)\n",
    "            critic_x_loss_steps.append(_critic_x_losses)\n",
    "\n",
    "            # Optimize step on critic z\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Do a step forward on critic z model and collect gradients\n",
    "                _critic_z_losses = self._critic_z_loss(x_mb, z, valid, fake, mini_batch_size)\n",
    "\n",
    "            # Backward step with updating critic z weights\n",
    "            critic_z_gradient = tape.gradient(_critic_z_losses[0], self.critic_z.trainable_variables)\n",
    "            self.critic_z_optimizer.apply_gradients(zip(critic_z_gradient, self.critic_z.trainable_variables))\n",
    "\n",
    "            # Collect summaries for logging\n",
    "            _critic_z_losses = np.array(_critic_z_losses)\n",
    "            critic_z_loss_steps.append(_critic_z_losses)\n",
    "\n",
    "        # Optimize step on encoder & generator and collect gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Do a step forward on the encoder generator model\n",
    "            _encoder_generator_losses = self._encoder_generator_loss(x_mb, z, valid)\n",
    "\n",
    "        # Backward step with updating encoder generator weights\n",
    "        encoder_generator_gradient = tape.gradient(_encoder_generator_losses, self.encoder.trainable_variables + self.generator.trainable_variables)\n",
    "        self.encoder_generator_optimizer.apply_gradients(zip(encoder_generator_gradient, self.encoder.trainable_variables + self.generator.trainable_variables))\n",
    "\n",
    "        # Collect summaries for logging\n",
    "        critic_x_losses = np.mean(np.array(critic_x_loss_steps), axis=0)\n",
    "        critic_z_losses = np.mean(np.array(critic_z_loss_steps), axis=0)\n",
    "        encoder_generator_losses = np.array(_encoder_generator_losses)\n",
    "\n",
    "        if self.log_all_losses:\n",
    "            loss_dict = {\n",
    "                \"Cx_total\": critic_x_losses[0],\n",
    "                \"Cx_valid\": critic_x_losses[1],\n",
    "                \"Cx_fake\": critic_x_losses[2],\n",
    "                \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "                \"Cz_total\": critic_z_losses[0],\n",
    "                \"Cz_valid\": critic_z_losses[1],\n",
    "                \"Cz_fake\": critic_z_losses[2],\n",
    "                \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "                \"EG_total\": encoder_generator_losses[0],\n",
    "                \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "                \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "                \"G_rec\": encoder_generator_losses[3],\n",
    "            }\n",
    "        else:\n",
    "            loss_dict = {\n",
    "                \"Cx_total\": critic_x_losses[0],\n",
    "                \"Cz_total\": critic_z_losses[0],\n",
    "                \"EG_total\": encoder_generator_losses[0]\n",
    "            }\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, X):\n",
    "        \"\"\"\n",
    "        Custom test step for this Subclassing API Keras model.\n",
    "        This overrides the default behavior of the model.evaluate() function.\n",
    "\n",
    "        :param X: Minibatch of time series signals (batch_size, signal_length, n_channels)\n",
    "\n",
    "        :return: Sub-model losses as los dict\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(X, tuple):\n",
    "            X = X[0]\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        # Prepare the ground truth data\n",
    "        fake = tf.ones((batch_size, 1))\n",
    "        valid = -tf.ones((batch_size, 1))\n",
    "\n",
    "        z = tf.random.normal(shape=(batch_size, self.latent_dim, 1))\n",
    "\n",
    "        critic_x_losses = self._critic_x_loss(X, z, valid, fake, batch_size)\n",
    "        critic_z_losses = self._critic_z_loss(X, z, valid, fake, batch_size)\n",
    "        encoder_generator_losses = self._encoder_generator_loss(X, z, valid)\n",
    "\n",
    "        if self.log_all_losses:\n",
    "            loss_dict = {\n",
    "                \"Cx_total\": critic_x_losses[0],\n",
    "                \"Cx_valid\": critic_x_losses[1],\n",
    "                \"Cx_fake\": critic_x_losses[2],\n",
    "                \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "                \"Cz_total\": critic_z_losses[0],\n",
    "                \"Cz_valid\": critic_z_losses[1],\n",
    "                \"Cz_fake\": critic_z_losses[2],\n",
    "                \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "                \"EG_total\": encoder_generator_losses[0],\n",
    "                \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "                \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "                \"G_rec\": encoder_generator_losses[3],\n",
    "            }\n",
    "        else:\n",
    "            loss_dict = {\n",
    "                \"Cx_total\": critic_x_losses[0],\n",
    "                \"Cz_total\": critic_z_losses[0],\n",
    "                \"EG_total\": encoder_generator_losses[0]\n",
    "            }\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, X, **kwargs) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "        \"\"\"\n",
    "        Default forward step during the inference step of the model (for training and evaluation see train_step() and test_step()).\n",
    "        This function will be called in the model.predict() function to use the trained sub networks for anomaly detection.\n",
    "\n",
    "        :param X: Batch of signals that should be analyzed by the model (batch_size, signal_length, n_channels)\n",
    "\n",
    "        :param kwargs: Additional kwargs forwarded to the super class fit method\n",
    "\n",
    "        :return: Tuple containing the outputs of the sub networks as numpy arrays:\n",
    "                 - The reconstructed signals from the generator\n",
    "                 - The compressed embedding of the time series (latent_dim, 1) from the encoder\n",
    "                 - The fake/real classification result for the reconstructed time series from the critic x network\n",
    "                 - The fake/real classification result for the learned embedding from the critic z network\n",
    "        \"\"\"\n",
    "        latent_encoding = self.encoder(X)\n",
    "        y_hat = self.generator(latent_encoding)\n",
    "        critic_x = self.critic_x(X)\n",
    "        critic_z = self.critic_z(latent_encoding)\n",
    "        return y_hat, latent_encoding, critic_x, critic_z\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "        validation_split=0., validation_data=None, shuffle=True,\n",
    "        class_weight=None, sample_weight=None,\n",
    "        initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "        max_queue_size=10, workers=1, use_multiprocessing=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extends the orignal fit method of the keras.Model API with some checks for the train_step batch size requirements.\n",
    "        See for https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit more details\n",
    "\n",
    "        :return: Keras model training history dict\n",
    "        \"\"\"\n",
    "        # Adjust batch size to support multiple training steps on the critics\n",
    "        if not isinstance(validation_data, tf.data.Dataset):\n",
    "            if (validation_data is not None) and (validation_batch_size is None):\n",
    "                validation_batch_size = batch_size\n",
    "\n",
    "        if not isinstance(x, tf.data.Dataset):\n",
    "            batch_size = batch_size * self.n_iterations_critic\n",
    "\n",
    "        return super().fit(x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle,\n",
    "                           class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps,\n",
    "                           validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n",
    "\n",
    "    @staticmethod\n",
    "    def export_as_keras_model(tadgan_model, export_path: str):\n",
    "        # Rebuild model graph\n",
    "        x = tf.keras.layers.Input(shape=tadgan_model.ts_input_shape, name=\"ts_input\")\n",
    "        latent_encoding = tadgan_model.encoder(x)\n",
    "        y_hat = tadgan_model.generator(latent_encoding)\n",
    "        critic_x = tadgan_model.critic_x(x)\n",
    "        critic_z = tadgan_model.critic_z(latent_encoding)\n",
    "\n",
    "        # Export model\n",
    "        standalone_model = tf.keras.models.Model(inputs=x, outputs=[y_hat, latent_encoding, critic_x, critic_z])\n",
    "        standalone_model.save(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153684e-7366-4b7c-812e-8c31c2255dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal = ... # Numpy array with training data\n",
    "x_val_normal = ... # Numpy array with validation data\n",
    "batch_size = 32 # Model batch size that should be used to fit the model\n",
    "predict_batch_size = 1024 # Bigger batch size to speed up model predictions\n",
    "\n",
    "train_normal_ds = tf.data.Dataset.from_tensor_slices(x_train_normal).shuffle(x_train_normal.shape[0])\n",
    "train_normal_ds = train_normal_ds.batch(batch_size * n_critic_train_iter, drop_remainder=True)\n",
    "\n",
    "val_normal_ds = tf.data.Dataset.from_tensor_slices(x_val_normal).shuffle(x_val_normal.shape[0])\n",
    "val_normal_ds = val_normal_ds.batch(predict_batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b907c-8a5b-43bc-b804-ed886d44696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "   train_normal_ds,\n",
    "   epochs=100,\n",
    "   validation_data=val_normal_ds\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
