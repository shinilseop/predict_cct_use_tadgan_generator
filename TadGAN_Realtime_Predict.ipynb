{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642f24cf-6fcb-49d2-975f-85a26b964298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import MongoDB as mdb\n",
    "import ETC_BaseModul as bm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import unroll_ts\n",
    "from model import hyperparameters\n",
    "from orion.primitives.tadgan import TadGAN\n",
    "import crawler_tosRS\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c0fa6f-06fe-4a64-9868-70a2989e292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"Aggregate values over given time span.\n",
    "    Args:\n",
    "        X (ndarray or pandas.DataFrame):\n",
    "            N-dimensional sequence of values.\n",
    "        interval (int):\n",
    "            Integer denoting time span to compute aggregation of.\n",
    "        time_column (int):\n",
    "            Column of X that contains time values.\n",
    "        method (str or list):\n",
    "            Optional. String describing aggregation method or list of strings describing multiple\n",
    "            aggregation methods. If not given, `mean` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray:\n",
    "            * Sequence of aggregated values, one column for each aggregation method.\n",
    "            * Sequence of index values (first index of each aggregated segment).\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    X = X.sort_values(time_column).set_index(time_column)\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    start_ts = X.index.values[0]\n",
    "    max_ts = X.index.values[-1]\n",
    "\n",
    "    values = list()\n",
    "    index = list()\n",
    "    while start_ts <= max_ts:\n",
    "        end_ts = start_ts + interval\n",
    "        subset = X.loc[start_ts:end_ts - 1]\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method\n",
    "        ]\n",
    "        values.append(np.concatenate(aggregated))\n",
    "        index.append(start_ts)\n",
    "        start_ts = end_ts\n",
    "\n",
    "    return np.asarray(values), np.asarray(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d284921a-351c-4a17-8126-61ca91aab639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
    "                             drop=None, drop_windows=False):\n",
    "    \"\"\"Create rolling window sequences out of time series data.\n",
    "    The function creates an array of input sequences and an array of target sequences by rolling\n",
    "    over the input sequence with a specified window.\n",
    "    Optionally, certain values can be dropped from the sequences.\n",
    "    Args:\n",
    "        X (ndarray):\n",
    "            N-dimensional sequence to iterate over.\n",
    "        index (ndarray):\n",
    "            Array containing the index values of X.\n",
    "        window_size (int):\n",
    "            Length of the input sequences.\n",
    "        target_size (int):\n",
    "            Length of the target sequences.\n",
    "        step_size (int):\n",
    "            Indicating the number of steps to move the window forward each round.\n",
    "        target_column (int):\n",
    "            Indicating which column of X is the target.\n",
    "        drop (ndarray or None or str or float or bool):\n",
    "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
    "            indicating which value should be dropped. If not given, `None` is used.\n",
    "        drop_windows (bool):\n",
    "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
    "            given, `False` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray, ndarray, ndarray:\n",
    "            * input sequences.\n",
    "            * target sequences.\n",
    "            * first index value of each input sequence.\n",
    "            * first index value of each target sequence.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb2861f-6ed5-4b52-aa25-495c2177f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_MAX_TIME = 3\n",
    "before_N = 're_2'\n",
    "\n",
    "target_N = 're_3'\n",
    "max_count = 10\n",
    "\n",
    "model_path = 'D:\\\\DeepLeaning\\\\TadGAN_model\\\\5-2. ver_15per(new)'\n",
    "model_name = '5_2_15per(new)'\n",
    "\n",
    "save_path = 'D:\\\\DeepLeaning\\\\TadGAN\\\\Shin\\\\Dataset\\\\realtime_test'\n",
    "target_date = '2017-05-17'\n",
    "save_name = 'result'\n",
    "\n",
    "# 시뮬레이션..이니깐\n",
    "year = target_date.split(\"-\")[0]\n",
    "month = target_date.split(\"-\")[1]\n",
    "day = target_date.split(\"-\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce5de85-d26a-4ac5-94db-b78616a7f0a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'predict_cct_use_tadgan_generatorchromedriver' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                             \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                             creationflags=self.creationflags)\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1206\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1208\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14952/2828297196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 실시간 최저점 찾기.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 찐 시작\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msunRS_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler_tosRS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtosRS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m AM_term = [12.67192225, 11.48028811, 12.10103828, 9.321626622, 10.58524961, 10.28859822, 14.85200124, 9.099921203,\n",
      "\u001b[1;32mD:\\DeepLeaning\\TadGAN_jupyter\\predict_cct_use_tadgan_generator\\crawler_tosRS.py\u001b[0m in \u001b[0;36mtosRS\u001b[1;34m(year, month, day)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtosRS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# print(\"[태양의 일출일몰 시간 수집]\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcralwer_tosRS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_URL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sunup_Time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sundown_Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DeepLeaning\\TadGAN_jupyter\\predict_cct_use_tadgan_generator\\crawler_tosRS.py\u001b[0m in \u001b[0;36mcralwer_tosRS\u001b[1;34m(url, year, month, day)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'headless'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'disable-gpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchrome_driver_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[0;32m     71\u001b[0m                                         \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                                         \u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_capabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                                         service_log_path, service, keep_alive)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TadGAN\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[1;32m---> 83\u001b[1;33m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[0;32m     84\u001b[0m                 )\n\u001b[0;32m     85\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'predict_cct_use_tadgan_generatorchromedriver' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n"
     ]
    }
   ],
   "source": [
    "# 실시간 최저점 찾기.\n",
    "# 찐 시작\n",
    "sunRS_df = crawler_tosRS.tosRS(year, month, day)\n",
    "\n",
    "AM_term = [12.67192225, 11.48028811, 12.10103828, 9.321626622, 10.58524961, 10.28859822, 14.85200124, 9.099921203,\n",
    "           9.895454429, 8.886125451, 11.34483191, 11.67173358]\n",
    "PM_term = [13.97183439, 12.1092905, 10.90653908, 8.674959456, 10.48454171, 10.1388775, 12.01368493, 7.212434035,\n",
    "           10.37609078, 7.496078472, 10.80341137, 13.25091053]\n",
    "\n",
    "date = str(sunRS_df['Date'][0])\n",
    "sunrise = str(sunRS_df['Sunup_Time'][0]).split(\".\")[0]\n",
    "sunset = str(sunRS_df['Sundown_Time'][0]).split(\".\")[0]\n",
    "\n",
    "# ori_date = year + \"/\" + month + \"/\" + day\n",
    "start_time = ori_date + \" \" + sunrise + \":00\"\n",
    "end_time = ori_date + \" \" + sunset + \":00\"\n",
    "\n",
    "print(start_time)\n",
    "start_time_obj = datetime.strptime(start_time, '%Y/%m/%d %H:%M:%S') - timedelta(hours=9)\n",
    "end_time_obj = datetime.strptime(end_time, '%Y/%m/%d %H:%M:%S') - timedelta(hours=9)\n",
    "\n",
    "day_len = (end_time_obj - start_time_obj).seconds / 60\n",
    "am_ratio = AM_term[int(month) - 1] / 100\n",
    "pm_ratio = PM_term[int(month) - 1] / 100\n",
    "am_cut_line = start_time_obj + timedelta(minutes=int(day_len * am_ratio))\n",
    "pm_cut_line = end_time_obj - timedelta(minutes=int(day_len * pm_ratio))\n",
    "start_time = start_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "end_time = end_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "# 최저 시작점 찾기\n",
    "min1 = 1000000\n",
    "count = 0\n",
    "start_min_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebed4c8-ff62-42d7-b683-11df23b26460",
   "metadata": {},
   "outputs": [],
   "source": [
    "while am_cut_line > start_time_obj:\n",
    "    step_times = mdb.make_ktc_to_utc_4step1(start_time)\n",
    "    step_data = mdb.load_last1_cct1(step_times)\n",
    "\n",
    "    step_df = mdb.mongodb_to_df(step_data, 'mongo_cas')\n",
    "    step_df = step_df.reset_index(drop=True)\n",
    "\n",
    "    if len(step_df) < 2:\n",
    "        break\n",
    "\n",
    "    step_df[\"CCT\"] = step_df[\"CCT\"].astype(float)\n",
    "    step_df[\"datetime\"] = step_df[\"datetime\"].astype(str)\n",
    "\n",
    "    start_time = step_df['datetime'].values[1].replace(\"-\", \"/\")\n",
    "    start_time_obj = datetime.strptime(start_time, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    now_cct = step_df['CCT'].values[1]\n",
    "    print(step_times, step_df)\n",
    "\n",
    "    if 6300 >= now_cct and now_cct >= 2500:\n",
    "        if min1 > now_cct:\n",
    "            min1 = now_cct\n",
    "            count = 0;\n",
    "            print(\"1\")\n",
    "            print(start_time, now_cct, min1, count)\n",
    "\n",
    "        else:\n",
    "            count = count + 1\n",
    "            print(\"2\", count)\n",
    "\n",
    "            # print(start_time, now_cct, min1, count)\n",
    "\n",
    "        if count == max_count:\n",
    "            start_time_obj = start_time_obj - timedelta(minutes=count)\n",
    "            start_time = start_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "            print(\"일출, 배치 최저점, 실시간 최저점\")\n",
    "            # print(start_time, now_cct, min1, count)\n",
    "            break\n",
    "\n",
    "    # 요것도 한번 고쳐야함.\n",
    "    elif am_cut_line < start_time_obj:\n",
    "        start_time_obj = start_time_obj - timedelta(minutes=count)\n",
    "        start_time = start_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "        print(\"일출, 배치 최저점, 실시간 최저점111\")\n",
    "        # print(start_time, now_cct, min1, count)\n",
    "        break\n",
    "\n",
    "    elif count > 0:\n",
    "        count = count + 1\n",
    "        # print(start_time, now_cct, min1, count)\n",
    "\n",
    "# 그걸로 부팅데이터를 만들어야함.\n",
    "# 부팅 순서는 아래와 같음. 100개 확보, 역포물선 만들기, 그걸로 tadgan 돌리기, -> 반복해서 n차까지 데이터 확보하기.\n",
    "\n",
    "# 최저점으로 부터 100개만 뽑기.\n",
    "print(start_time, end_time)\n",
    "boot_times = mdb.make_ktc_to_utc_4boot(start_time)\n",
    "boot_data = mdb.load_last100_cct(boot_times)\n",
    "boot_df = mdb.mongodb_to_df(boot_data, 'mongo_cas')\n",
    "print(boot_df)\n",
    "boot_df[\"CCT\"] = boot_df[\"CCT\"].astype(float)\n",
    "# boot_df.to_csv(\"boot_df_df.csv\", sep=',', na_rep='NaN')\n",
    "\n",
    "# 역포물선 만들기\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "temp_df = boot_df.reset_index(drop=True)\n",
    "temp_df['Timestamp'] = temp_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "temp_df = temp_df[['Timestamp', 'CCT']]\n",
    "temp_np = temp_df.values\n",
    "temp_np = scaler.fit_transform(temp_np)\n",
    "temp_df = pd.DataFrame(temp_np, columns=['Timestamp', 'input'])\n",
    "\n",
    "reverse_df = bm.make_underzero_reverse_df(temp_df.copy(), \"input\")\n",
    "total_df = reverse_df.append(temp_df)\n",
    "total_df = total_df.reset_index(drop=True)\n",
    "# 원본 시간하고 색온도 추가해서 저장하기.\n",
    "for i in range(100, 200):\n",
    "    total_df.loc[i, \"datetime\"] = boot_df['datetime'][i - 100:i - 99].values[0]\n",
    "    total_df.loc[i, \"ori_cct\"] = boot_df['CCT'][i - 100:i - 99].values[0]\n",
    "total_df = total_df[['datetime', 'ori_cct', 'Timestamp', 'input']]\n",
    "total_df['Timestamp'] = total_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "\n",
    "# 모델 세팅\n",
    "hyperparameters[\"epochs\"] = 100\n",
    "hyperparameters[\"shape\"] = (100, 1)  # based on the window size\n",
    "hyperparameters[\"optimizer\"] = \"keras.optimizers.Adam\"\n",
    "hyperparameters[\"learning_rate\"] = 0.0005\n",
    "hyperparameters[\"latent_dim\"] = 20\n",
    "hyperparameters[\"batch_size\"] = 64\n",
    "tgan = TadGAN(**hyperparameters)\n",
    "\n",
    "tgan._build_tadgan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78193af7-66d9-4e0f-aad4-5a0c3c810ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgan.encoder.load_weights(f\"{model_path}\\\\encoder_save_weights_{model_name}.h5\")\n",
    "tgan.generator.load_weights(f\"{model_path}\\\\generator_save_weights_{model_name}.h5\")\n",
    "tgan.critic_x.load_weights(f\"{model_path}\\\\critic_x_save_weights_{model_name}.h5\")\n",
    "tgan.critic_z.load_weights(f\"{model_path}\\\\critic_z_save_weights_{model_name}.h5\")\n",
    "\n",
    "if model_type == \"2%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per\\\\model\\\\encoder_save_weights_5_2_2per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per\\\\model\\\\generator_save_weights_5_2_2per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per\\\\model\\\\critic_x_save_weights_5_2_2per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per\\\\model\\\\critic_z_save_weights_5_2_2per.h5\")\n",
    "\n",
    "elif model_type == \"0%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. under_reverse_oneday_sc_insert_gap\\\\model\\\\encoder_save_weights_under_reverse_oneday_sc_insert_gap_ver2.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. under_reverse_oneday_sc_insert_gap\\\\model\\\\generator_save_weights_under_reverse_oneday_sc_insert_gap_ver2.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. under_reverse_oneday_sc_insert_gap\\\\model\\\\critic_x_save_weights_under_reverse_oneday_sc_insert_gap_ver2.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. under_reverse_oneday_sc_insert_gap\\\\model\\\\critic_z_save_weights_under_reverse_oneday_sc_insert_gap_ver2.h5\")\n",
    "\n",
    "elif model_type == \"5%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per\\\\model\\\\encoder_save_weights_5_2_5per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per\\\\model\\\\generator_save_weights_5_2_5per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per\\\\model\\\\critic_x_save_weights_5_2_5per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per\\\\model\\\\critic_z_save_weights_5_2_5per.h5\")\n",
    "\n",
    "elif model_type == \"10%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per\\\\model\\\\encoder_save_weights_5_2_10per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per\\\\model\\\\generator_save_weights_5_2_10per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per\\\\model\\\\critic_x_save_weights_5_2_10per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per\\\\model\\\\critic_z_save_weights_5_2_10per.h5\")\n",
    "\n",
    "elif model_type == \"20%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per\\\\model\\\\encoder_save_weights_5_2_20per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per\\\\model\\\\generator_save_weights_5_2_20per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per\\\\model\\\\critic_x_save_weights_5_2_20per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per\\\\model\\\\critic_z_save_weights_5_2_20per.h5\")\n",
    "\n",
    "elif model_type == \"30%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per\\\\model\\\\encoder_save_weights_5_2_30per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per\\\\model\\\\generator_save_weights_5_2_30per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per\\\\model\\\\critic_x_save_weights_5_2_30per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per\\\\model\\\\critic_z_save_weights_5_2_30per.h5\")\n",
    "\n",
    "elif model_type == \"60%\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_60per\\\\model\\\\encoder_save_weights_5_2_60per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_60per\\\\model\\\\generator_save_weights_5_2_60per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_60per\\\\model\\\\critic_x_save_weights_5_2_60per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_60per\\\\model\\\\critic_z_save_weights_5_2_60per.h5\")\n",
    "\n",
    "elif model_type == \"15%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(new)\\\\model\\\\encoder_save_weights_5_2_15per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(new)\\\\model\\\\generator_save_weights_5_2_15per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(new)\\\\model\\\\critic_x_save_weights_5_2_15per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(new)\\\\model\\\\critic_z_save_weights_5_2_15per(new).h5\")\n",
    "\n",
    "elif model_type == \"15%_all\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per\\\\model\\\\encoder_save_weights_5_2_15per.h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per\\\\model\\\\generator_save_weights_5_2_15per.h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per\\\\model\\\\critic_x_save_weights_5_2_15per.h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per\\\\model\\\\critic_z_save_weights_5_2_15per.h5\")\n",
    "\n",
    "elif model_type == \"15%_old\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(old)\\\\model\\\\encoder_save_weights_5_2_15per(old).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(old)\\\\model\\\\generator_save_weights_5_2_15per(old).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(old)\\\\model\\\\critic_x_save_weights_5_2_15per(old).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_15per(old)\\\\model\\\\critic_z_save_weights_5_2_15per(old).h5\")\n",
    "\n",
    "elif model_type == \"10%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(new)\\\\model\\\\encoder_save_weights_5_2_10per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(new)\\\\model\\\\generator_save_weights_5_2_10per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(new)\\\\model\\\\critic_x_save_weights_5_2_10per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(new)\\\\model\\\\critic_z_save_weights_5_2_10per(new).h5\")\n",
    "\n",
    "elif model_type == \"10%_all2\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(all2)\\\\model\\\\encoder_save_weights_5_2_10per(all2).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(all2)\\\\model\\\\generator_save_weights_5_2_10per(all2).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(all2)\\\\model\\\\critic_x_save_weights_5_2_10per(all2).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(all2)\\\\model\\\\critic_z_save_weights_5_2_10per(all2).h5\")\n",
    "\n",
    "elif model_type == \"10%_old\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(old)\\\\model\\\\encoder_save_weights_5_2_10per(old).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(old)\\\\model\\\\generator_save_weights_5_2_10per(old).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(old)\\\\model\\\\critic_x_save_weights_5_2_10per(old).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_10per(old)\\\\model\\\\critic_z_save_weights_5_2_10per(old).h5\")\n",
    "\n",
    "elif model_type == \"50%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(new)\\\\model\\\\encoder_save_weights_5_2_50per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(new)\\\\model\\\\generator_save_weights_5_2_50per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(new)\\\\model\\\\critic_x_save_weights_5_2_50per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(new)\\\\model\\\\critic_z_save_weights_5_2_50per(new).h5\")\n",
    "\n",
    "elif model_type == \"50%_all\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(all)\\\\model\\\\encoder_save_weights_5_2_50per(all).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(all)\\\\model\\\\generator_save_weights_5_2_50per(all).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(all)\\\\model\\\\critic_x_save_weights_5_2_50per(all).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(all)\\\\model\\\\critic_z_save_weights_5_2_50per(all).h5\")\n",
    "\n",
    "elif model_type == \"50%_old\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(old)\\\\model\\\\encoder_save_weights_5_2_50per(old).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(old)\\\\model\\\\generator_save_weights_5_2_50per(old).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(old)\\\\model\\\\critic_x_save_weights_5_2_50per(old).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_50per(old)\\\\model\\\\critic_z_save_weights_5_2_50per(old).h5\")\n",
    "\n",
    "elif model_type == \"0%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_0per(new)\\\\model\\\\encoder_save_weights_5_2_0per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_0per(new)\\\\model\\\\generator_save_weights_5_2_0per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_0per(new)\\\\model\\\\critic_x_save_weights_5_2_0per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_0per(new)\\\\model\\\\critic_z_save_weights_5_2_0per(new).h5\")\n",
    "\n",
    "elif model_type == \"2%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per(new)\\\\model\\\\encoder_save_weights_5_2_2per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per(new)\\\\model\\\\generator_save_weights_5_2_2per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per(new)\\\\model\\\\critic_x_save_weights_5_2_2per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_2per(new)\\\\model\\\\critic_z_save_weights_5_2_2per(new).h5\")\n",
    "\n",
    "elif model_type == \"5%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per(new)\\\\model\\\\encoder_save_weights_5_2_5per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per(new)\\\\model\\\\generator_save_weights_5_2_5per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per(new)\\\\model\\\\critic_x_save_weights_5_2_5per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_5per(new)\\\\model\\\\critic_z_save_weights_5_2_5per(new).h5\")\n",
    "\n",
    "elif model_type == \"20%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per(new)\\\\model\\\\encoder_save_weights_5_2_20per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per(new)\\\\model\\\\generator_save_weights_5_2_20per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per(new)\\\\model\\\\critic_x_save_weights_5_2_20per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_20per(new)\\\\model\\\\critic_z_save_weights_5_2_20per(new).h5\")\n",
    "\n",
    "elif model_type == \"30%_new\":\n",
    "    tgan.encoder.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per(new)\\\\model\\\\encoder_save_weights_5_2_30per(new).h5\")\n",
    "    tgan.generator.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per(new)\\\\model\\\\generator_save_weights_5_2_30per(new).h5\")\n",
    "    tgan.critic_x.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per(new)\\\\model\\\\critic_x_save_weights_5_2_30per(new).h5\")\n",
    "    tgan.critic_z.load_weights(\n",
    "        \"C:\\\\Users\\\\GAKA\\\\Desktop\\\\TadGAN\\\\5-2. ver_30per(new)\\\\model\\\\critic_z_save_weights_5_2_30per(new).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959242f1-9ff4-49a6-a452-189a1f2dd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  기존엔 스케일을 한번에 0,1 로 줄여서 반복후에 스케일업, 역스케일링을 했다.\n",
    "#  실시간으로 하게되면 1회 실행했을 때 이후 들어오는 데이터가 더 높은 값일 수도 있기 때문에 다시 스케일을 맞춰야 한다.\n",
    "for i in range(RE_MAX_TIME):\n",
    "    X, index = time_segments_aggregate(total_df[['Timestamp', 'input']], interval=1, time_column='Timestamp')\n",
    "    imp = SimpleImputer()\n",
    "    X = imp.fit_transform(X)\n",
    "\n",
    "    X, y, X_index, y_index = rolling_window_sequences(X, index,\n",
    "                                                      window_size=100,\n",
    "                                                      target_size=1,\n",
    "                                                      step_size=1,\n",
    "                                                      target_column=0)\n",
    "\n",
    "    # 학습된 모델을 데이터에 적용\n",
    "    X_hat, critic = tgan.predict(X)\n",
    "    y_hat = unroll_ts(X_hat)\n",
    "    # plot_ts([y, y_hat], labels=['original', 'reconstructed'])\n",
    "\n",
    "    total_df.loc[100:200, \"tadgan_temp\"] = y_hat\n",
    "    # print(y_hat)\n",
    "    #\n",
    "\n",
    "    # 역스케일링 하고, re_n차에 저장, 그걸 가지고 다시 0,1 스케일링, 역포물선 제작, input으로 넣기\n",
    "    if i == 0:\n",
    "        max_cct = np.nanmax(total_df['ori_cct'].values)\n",
    "        min_cct = np.nanmin(total_df['ori_cct'].values)\n",
    "    else:\n",
    "        max_cct = np.nanmax(total_df['re_' + str(i)].values)\n",
    "        min_cct = np.nanmin(total_df['re_' + str(i)].values)\n",
    "\n",
    "    total_df['re_' + str(i)] = total_df.apply(lambda x: (x['input'] * (max_cct - min_cct)) + min_cct, axis=1)\n",
    "    total_df['re_' + str(i + 1)] = total_df.apply(lambda x: (x['tadgan_temp'] * (max_cct - min_cct)) + min_cct,\n",
    "                                                  axis=1)\n",
    "\n",
    "    reverse_df = bm.make_Relatively_reverse_df(total_df.copy(), 're_' + str(i + 1))\n",
    "    reverse_df = reverse_df.reset_index(drop=True)\n",
    "\n",
    "    temp_max = np.nanmax(reverse_df['re_' + str(i + 1)].values)\n",
    "    temp_min = np.nanmin(total_df['re_' + str(i + 1)].values)\n",
    "    reverse_df['re_' + str(i + 1)] = reverse_df.apply(lambda x: x['re_' + str(i + 1)] - (temp_max - temp_min),\n",
    "                                                      axis=1)\n",
    "\n",
    "    total_df.loc[0:99, 're_' + str(i + 1)] = reverse_df['re_' + str(i + 1)][0:]\n",
    "\n",
    "    temp_df = total_df[['re_' + str(i + 1)]]\n",
    "    temp_np = temp_df.values\n",
    "    temp_np = scaler.fit_transform(temp_np)\n",
    "    temp_df = pd.DataFrame(temp_np, columns=['input'])\n",
    "    total_df.loc[0:, \"input\"] = temp_df[\"input\"][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24f5d9-0ebb-439f-a080-dd2742a35e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  부팅 데이터 적용하고, 그거 기준으로 스케일 업 하는 횟수 정하고, 역스케일링 하고 반복.\n",
    "\n",
    "# 스케일 업하는 기준 잡기(지수)\n",
    "total_df['up_sc'] = total_df.apply(lambda x: x[before_N] / x[target_N], axis=1)\n",
    "\n",
    "#\n",
    "sc_k_pd = pd.DataFrame()\n",
    "sc_k_pd['re_0'] = total_df['re_0']\n",
    "\n",
    "row1 = []\n",
    "row2 = []\n",
    "for j in range(41):\n",
    "    # 각 업스케일링 회차에 대한 성능평가 진행하여 직전 대체값에서 얼마나 변화가 적은지 판단하고 삽입. 반복.\n",
    "\n",
    "    SC_K = 0.1 * j\n",
    "    sc_k_pd[str(SC_K)] = total_df.apply(lambda x: (float(x[target_N]) * pow(float(x['up_sc']), SC_K)), axis=1)\n",
    "    # print(sc_k_pd[str(SC_K)])\n",
    "    sc_k_pd[str(SC_K) + \"_AE\"] = total_df.apply(lambda x: 0, axis=1)\n",
    "\n",
    "    for i in range(1, len(sc_k_pd)):\n",
    "        sc_k_pd.loc[i, str(SC_K) + '_AE'] = abs((float(sc_k_pd['re_0'][i]) - (float(sc_k_pd[str(SC_K)][i]))))\n",
    "    #\n",
    "    row1.append(SC_K)\n",
    "    row2.append(np.mean(sc_k_pd[str(SC_K) + '_AE'][99:].to_numpy()))\n",
    "\n",
    "    # plt.scatter(range(len(sc_k_pd[99:])), sc_k_pd['re_0'][99:], color='red', marker='o', s=1)\n",
    "    # plt.scatter(range(len(sc_k_pd[99:])), sc_k_pd[str(SC_K)][99:], color='blue', marker='o', s=1)\n",
    "    # plt.title(str(SC_K))\n",
    "    # plt.savefig(\"C:\\\\Users\\\\GAKA\\\\Desktop\\\\workspace\\\\Python\\\\TadGAN_final\\\\\\RealTime_system\\\\plt\\\\\"+str(SC_K)+\".jpg\")\n",
    "    # plt.close(\"all\")\n",
    "\n",
    "index = row2.index(min(row2))\n",
    "SC_K = row1[index]\n",
    "print(SC_K)\n",
    "total_df['output'] = total_df.apply(lambda x: x[target_N] * pow(x['up_sc'], SC_K), axis=1)\n",
    "total_df = total_df.drop(columns=['input', 'tadgan_temp'])\n",
    "total_df.to_csv(\"booting.csv\", sep=',')\n",
    "\n",
    "# return 0\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "start_time = total_df['datetime'].values[-1]\n",
    "time = str(start_time)\n",
    "temp = time.split(\"T\")\n",
    "date = temp[0].split(\"-\")\n",
    "\n",
    "ori_date = date[0] + \"/\" + date[1] + \"/\" + date[2]\n",
    "start_time = ori_date + \" \" + temp[1].split(\".\")[0]\n",
    "start_time_obj = datetime.strptime(start_time, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# 이제 데이터 1개씩 끌어와서 반복 실행.\n",
    "#\n",
    "j = 100\n",
    "min1 = 1000000\n",
    "count = 0\n",
    "find_endpoint = False\n",
    "end_point = 0\n",
    "# total_df['J_index'] = temp_df.apply(lambda x: 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b203b7b-42ea-4db0-acf5-ce9848b5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # start_time_obj를 증가해시켜야함.(맨뒤에서)\n",
    "    if (find_endpoint==False):\n",
    "        if 1 > ((end_time_obj - start_time_obj).seconds / 60):\n",
    "            print(\"그냥 다돔\")\n",
    "            break\n",
    "\n",
    "        elif end_time_obj < start_time_obj:\n",
    "            print(\"그냥 다돔1\", j, start_time_obj)\n",
    "            break\n",
    "\n",
    "        step_times = mdb.make_ktc_to_utc_4step1(start_time)\n",
    "        step_data = mdb.load_last1_cct1(step_times)\n",
    "\n",
    "        step_df = mdb.mongodb_to_df(step_data, 'mongo_cas')\n",
    "        step_df = step_df.reset_index(drop=True)\n",
    "\n",
    "        if len(step_df) < 2:\n",
    "            break\n",
    "\n",
    "        step_df[\"CCT\"] = step_df[\"CCT\"].astype(float)\n",
    "        step_df[\"datetime\"] = step_df[\"datetime\"].astype(str)\n",
    "\n",
    "        # 요건 밑으로 이전해야 할 수도 있고 알고리즘 내부에서 실행 해야 할 수도 있음.\n",
    "        start_time = step_df['datetime'].values[1].replace(\"-\", \"/\")\n",
    "        start_time_obj = datetime.strptime(start_time, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "        CCT = step_df['CCT'].values[1]\n",
    "\n",
    "        if count < max_count:\n",
    "            total_df.loc[j + 100, \"ori_cct\"] = CCT\n",
    "            total_df.loc[j + 100, \"re_0\"] = CCT\n",
    "            total_df.loc[j + 100, \"datetime\"] = start_time\n",
    "            total_df['Timestamp'] = temp_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "\n",
    "    # print(start_time_obj)\n",
    "    if (pm_cut_line <= start_time_obj):\n",
    "        # 의심구간 처음 들어옴\n",
    "\n",
    "        if 1 > abs((pm_cut_line - start_time_obj).seconds / 60):\n",
    "            if 6300 < CCT and CCT < 2500:\n",
    "                min1 = CCT\n",
    "                count = count + 1\n",
    "        elif count > 0 and 6300 < CCT and CCT < 2500:\n",
    "            count = count + 1\n",
    "            if count == max_count:\n",
    "                end_time_obj = start_time_obj - timedelta(minutes=count)\n",
    "                end_time = end_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "                print(\"일몰, 배치 최저점, 실시간 최저점1\")\n",
    "                find_endpoint = True\n",
    "                end_point = j + 100 - k\n",
    "                print(end_time)\n",
    "                count = count + 1\n",
    "                for k in range(max_count):\n",
    "                    total_df = total_df.drop([j + 100 - k])\n",
    "                temp = bm.make_Relatively_reverse_df_4ending(total_df)\n",
    "\n",
    "                input_count = 0\n",
    "                for i in range(len(total_df), len(total_df) + 100):\n",
    "                    total_df.loc[i, \"ori_cct\"] = temp['CCT'][input_count]\n",
    "                    total_df.loc[i, \"re_0\"] = temp['CCT'][input_count]\n",
    "                    input_count = input_count + 1\n",
    "                total_df['Timestamp'] = temp_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "                # break\n",
    "\n",
    "        if 6300 >= CCT and CCT >= 2500 and count < max_count:\n",
    "            if min1 > CCT:\n",
    "                min1 = CCT\n",
    "                count = 0\n",
    "\n",
    "            else:\n",
    "                count = count + 1\n",
    "\n",
    "            if count == max_count:\n",
    "                end_time_obj = start_time_obj - timedelta(minutes=count)\n",
    "                end_time = end_time_obj.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "                print(\"일몰, 배치 최저점, 실시간 최저점2\")\n",
    "                find_endpoint = True\n",
    "                end_point = j + 100 - k\n",
    "                print(end_time)\n",
    "                # print(total_df)\n",
    "\n",
    "                count = count + 1\n",
    "                for k in range(max_count):\n",
    "                    total_df = total_df.drop([j + 100 - k])\n",
    "                temp = bm.make_Relatively_reverse_df_4ending(total_df)\n",
    "\n",
    "                input_count = 0\n",
    "                for i in range(len(total_df), len(total_df) + 100):\n",
    "                    total_df.loc[i, \"ori_cct\"] = temp['CCT'][input_count]\n",
    "                    total_df.loc[i, \"re_0\"] = temp['CCT'][input_count]\n",
    "                    input_count = input_count + 1\n",
    "                total_df['Timestamp'] = temp_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "                # break\n",
    "\n",
    "    # 임시 점프구간\n",
    "    # j = j+1\n",
    "    # continue\n",
    "    #\n",
    "\n",
    "    # 모델 반복 적용\n",
    "    # total_df['J_index'][j] = j\n",
    "    indexing = 100\n",
    "    if find_endpoint:\n",
    "        if pd.isnull(total_df.loc[j, 'datetime']):\n",
    "            print(total_df.loc[j,'datetime'])\n",
    "            print(j, end_point)\n",
    "            break\n",
    "\n",
    "    # print(j, len(temp_df))\n",
    "    for i in range(RE_MAX_TIME):\n",
    "        temp_df = total_df.loc[j - indexing: j + indexing, ['Timestamp', 're_' + str(i)]]\n",
    "        # print(j, len(temp_df))\n",
    "        # if len(temp_df) != 201:\n",
    "        #     print(j, len(temp_df))\n",
    "            # 백개보다 아래로 내려가면 멈추긴해야지.. 왜냐하면 앤딩데이터가 만들어주니깐..~! 이건 유지.\n",
    "            # break\n",
    "\n",
    "        max_cct = np.nanmax(temp_df['re_' + str(i)].values)\n",
    "        min_cct = np.nanmin(temp_df['re_' + str(i)].values)\n",
    "\n",
    "        temp_np = temp_df.values\n",
    "        temp_np = scaler.fit_transform(temp_np)\n",
    "        temp_df = pd.DataFrame(temp_np, columns=['Timestamp', 'input'])\n",
    "        temp_df['Timestamp'] = temp_df.apply(lambda x: float(x.name) + 1, axis=1)\n",
    "        # print(len(temp_df))\n",
    "        X, index = time_segments_aggregate(temp_df, interval=1, time_column='Timestamp')\n",
    "        imp = SimpleImputer()\n",
    "        X = imp.fit_transform(X)\n",
    "        # print(X)\n",
    "        X, y, X_index, y_index = rolling_window_sequences(X, index,\n",
    "                                                          window_size=100,\n",
    "                                                          target_size=1,\n",
    "                                                          step_size=1,\n",
    "                                                          target_column=0)\n",
    "\n",
    "        # 학습된 모델을 데이터에 적용\n",
    "        X_hat, critic = tgan.predict(X)\n",
    "        y_hat = unroll_ts(X_hat)\n",
    "        # print(len(y_hat))\n",
    "        # plot_ts([y, y_hat], labels=['original', 'reconstructed'])\n",
    "\n",
    "        # 스케일 업해서. 저장해야함.\n",
    "\n",
    "        # y hat이 j - indexing-100 개만큼 있음. total[j - indexing+100:] 범위겠지.\n",
    "        for k in range(len(y_hat)):\n",
    "            temp_cct = (y_hat[k] * (max_cct - min_cct)) + min_cct\n",
    "            total_df.loc[j + k, 're_' + str(i + 1)] = temp_cct\n",
    "\n",
    "        # temp_cct = (y_hat[0] * (max_cct - min_cct)) + min_cct\n",
    "        # total_df.loc[j, 're_' + str(i + 1)] = temp_cct\n",
    "        # print(max_cct,min_cct,y_hat,temp_cct)\n",
    "\n",
    "    if len(temp_df) > 100:\n",
    "        # 산출물이 100개 이상일 때에만 결과물을 넣는다..? 근데 이게 꼭 필요한지는 의문 .,., 2중 보호같이 되어있어서\n",
    "        total_df.loc[j, 'up_sc'] = total_df.loc[j, before_N] / total_df.loc[j, target_N]\n",
    "        total_df.loc[j, 'output'] = total_df.loc[j, target_N] * pow(total_df.loc[j, 'up_sc'], SC_K)\n",
    "\n",
    "    print(j)\n",
    "    # if j%20==0:\n",
    "    #     plt.scatter(range(j), total_df['log_5'][:j], color='red', marker='o', s=1)\n",
    "    #     plt.scatter(range(j), total_df['ori_cct'][:j], color='blue', marker='o', s=1)\n",
    "    #     plt.title('log_5_'+str(j))\n",
    "    #     plt.savefig(\"C:\\\\Users\\\\GAKA\\\\Desktop\\\\workspace\\\\Python\\\\TadGAN_final\\\\\\RealTime_system\\\\plt\\\\\" + 'log_5_'+str(j) + \".jpg\")\n",
    "    #     plt.close(\"all\")\n",
    "    j = j + 1\n",
    "\n",
    "# print(total_df)\n",
    "# 이건 마지막 끝나고 잔여 데이터 100개 넣어주는 행위 / 삭제 예정 : 앤딩 데이터셋이 있으니깐.!\n",
    "\n",
    "\n",
    "# for q in range(j, len(total_df)):\n",
    "#     total_df.loc[q, 'up_sc'] = total_df.loc[q, before_N] / total_df.loc[q, target_N]\n",
    "#     total_df.loc[q, 'output'] = total_df.loc[q, target_N] * pow(total_df.loc[q, 'up_sc'], SC_K)\n",
    "total_df = total_df[total_df['datetime'].notnull()]\n",
    "\n",
    "total_df.to_csv(\"./result/\" + ori_date.replace(\"/\", \"-\") + \"_\" + model_type + \".csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
